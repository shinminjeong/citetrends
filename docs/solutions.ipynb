{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CML Winter Workshop Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from IPython.display import display, HTML\n",
    "from scipy.spatial.distance import squareform\n",
    "from sklearn import datasets, manifold\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from torch import LongTensor, FloatTensor\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu = torch.device('cpu')\n",
    "# Randomly pick one of the two GPUs on braun\n",
    "gpu_no = np.random.choice([0, 2])\n",
    "gpu = torch.device('cuda:{}'.format(gpu_no))\n",
    "\n",
    "print('We are using torch v{}'.format(torch.__version__))\n",
    "print('GPU is available:', torch.cuda.is_available())\n",
    "print('Using GPU {}'.format(gpu_no))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 1. Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_matrix(matrix):\n",
    "    \"\"\" Implement your code below to return the square of a matrix. \"\"\"\n",
    "    return torch.mm(matrix, matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to see how much faster we can square a matrix in a GPU compared to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_cpu_vs_gpu(sizes, operation, gpu):\n",
    "    gpu_times = []\n",
    "    cpu_times = []\n",
    "    for size in tqdm(sizes):\n",
    "        matrix = torch.rand(size, size)\n",
    "\n",
    "        start = time()\n",
    "        operation(matrix.to(gpu))\n",
    "        gpu_times.append(time() - start)\n",
    "\n",
    "        start = time()\n",
    "        operation(matrix.to(torch.device('cpu')))\n",
    "        cpu_times.append(time() - start)\n",
    "    construct_results(sizes, cpu_times, gpu_times)\n",
    "\n",
    "\n",
    "def construct_results(sizes, cpu_times, gpu_times):\n",
    "    diff = np.round(np.array(cpu_times) / np.array(gpu_times), 3)\n",
    "    data = OrderedDict({'GPU Time': gpu_times,\n",
    "                        'CPU Time': cpu_times,\n",
    "                        'Difference': diff})\n",
    "    df = pd.DataFrame(data, index=pd.Index(sizes, name='Size'))\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1, 10, 100, 1000, 2000, 4000, 8000]\n",
    "compare_cpu_vs_gpu(sizes, square_matrix, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 2. Finding Matrix Inverse\n",
    "\n",
    "Implement a function to find the inverse of matrix, using `torch.inverse()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_inverse(matrix):\n",
    "    \"\"\" Implement your code below to return the inverse of a matrix. \"\"\"\n",
    "    return torch.inverse(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to see how much faster we can take an invesre of a matrix in a GPU compared to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1, 10, 100, 1000, 2000, 4000, 8000]\n",
    "compare_cpu_vs_gpu(sizes, find_inverse, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 3. Singular Value Decomposition\n",
    "\n",
    "\n",
    "Implement a function to do SVD. Use `torch.svd()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_svd(matrix):\n",
    "    \"\"\" Implement your code below to return the SVD of a matrix. \"\"\"\n",
    "    u, s, v = torch.svd(matrix)\n",
    "    return u, s, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [1, 10, 100, 1000, 2000, 4000]\n",
    "compare_cpu_vs_gpu(sizes, perform_svd, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a technique to visualize high-dimensional data by giving each datapoint a location in a two or three-dimensional map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the MNIST dataset with only 6 classes (digit 0 - 5)\n",
    "digits = datasets.load_digits(n_class=6)\n",
    "\n",
    "# X has shape (1083, 64). There are 1083 images, each with 64 pixels\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "for i in range(1, 26):\n",
    "    ax = fig.add_subplot(5, 5, i)\n",
    "    idx = np.random.randint(0, len(X))\n",
    "    ax.imshow(X[idx].reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want the joint probabilities in the high dimensional space.\n",
    "\n",
    "$$\n",
    "{\\displaystyle p_{j\\mid i}={\\frac {\\exp(-\\lVert \\mathbf {x} _{i}-\\mathbf {x} _{j}\\rVert ^{2}/2\\sigma _{i}^{2})}{\\sum _{k\\neq i}\\exp(-\\lVert \\mathbf {x} _{i}-\\mathbf {x} _{k}\\rVert ^{2}/2\\sigma _{i}^{2})}},} \\\\\n",
    "{\\displaystyle p_{ij}={\\frac {p_{j\\mid i}+p_{i\\mid j}}{2N}}}\n",
    "$$\n",
    "\n",
    "To save time, the impelentation of $p_{ij}$ is given below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pij(X):\n",
    "    \"\"\" Returns three variables. pij are the probabibilties flattened out into a vector.\n",
    "        i and j are the indices that allow us to recover the original position.\n",
    "        For example, pij[100] is the probability porportional to the simialirity\n",
    "        between point i[100] and point j[100]\n",
    "    \"\"\"\n",
    "    # Compute the L2-distance between all pairs of point\n",
    "    distance_matrix = pairwise_distances(X, metric='euclidean', squared=True)\n",
    "\n",
    "    # manifold._joint_probabilities() returns a condensed distance matrix (the upper\n",
    "    # triangle flattened out as a long vector)\n",
    "    pij = manifold.t_sne._joint_probabilities(\n",
    "        distance_matrix, desired_perplexity=30, verbose=False)\n",
    "\n",
    "    # Convert into a full distance matrix. pij will now be a (1083, 1083) matrix of the\n",
    "    # join probabilities p_ij\n",
    "    pij = squareform(pij)\n",
    "\n",
    "    # Remove self-indices\n",
    "    i, j = np.indices(pij.shape)\n",
    "    i = i.ravel()\n",
    "    j = j.ravel()\n",
    "    pij = pij.ravel().astype('float32')\n",
    "    idx = i != j\n",
    "    i, j, pij = i[idx], j[idx], pij[idx]\n",
    "    \n",
    "    return i, j, pij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j, pij = calculate_pij(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want the joint probabiltiies in the low dimensional space:\n",
    "\n",
    "$$\n",
    "q_{ij}={\\frac {(1+\\lVert \\mathbf {y} _{i}-\\mathbf {y} _{j}\\rVert ^{2})^{-1}}{\\sum _{k\\neq l}(1+\\lVert \\mathbf {y} _{k}-\\mathbf {y} _{l}\\rVert ^{2})^{-1}}}\n",
    "$$\n",
    "\n",
    "and then we want to minimise the KL-divergence between $p_{ij}$ and $q_{ij}$:\n",
    "\n",
    "$$\n",
    "{\\displaystyle KL(P||Q)=\\sum _{i\\neq j}p_{ij}\\log {\\frac {p_{ij}}{q_{ij}}}}\n",
    "$$\n",
    "\n",
    "Here are some useful functions to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = torch.arange(1.0, 7.0).view(2, 3)\n",
    "print(matrix)\n",
    "\n",
    "# Get the dimensions of the matrix\n",
    "print(matrix.size())\n",
    "\n",
    "# Add an extra dimension\n",
    "print(matrix.unsqueeze(0))\n",
    "\n",
    "# Expand matrix to a larger size\n",
    "print(matrix.unsqueeze(0).expand(4, 2, 3))\n",
    "\n",
    "# Sum up along the first dimension\n",
    "print(matrix.sum(1))\n",
    "\n",
    "# Sum up all elements in a matrix\n",
    "print(matrix.sum())\n",
    "\n",
    "# Compute element-wise reciprocal\n",
    "print(matrix.pow(-1))\n",
    "\n",
    "# Finding the log\n",
    "print(torch.log(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 4. KL-divergence\n",
    "Implement a function to compute the KL-divergence between two distribution $p_{ij}$ and $q_{ij}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(pij, qij):\n",
    "    \"\"\" Return the KL loss as a scalar. \"\"\"\n",
    "    loss_kld = pij * (torch.log(pij) - torch.log(qij))\n",
    "    return loss_kld.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "pij_test = FloatTensor([0.2, 0.4, 0.2, 0.2])\n",
    "qij_test = FloatTensor([0.1, 0.3, 0.5, 0.1])\n",
    "assert kl_loss(pij_test, qij_test).item() - 0.2091 < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 5. Computing Distance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing $q_{ij}$, we first need to have the distance matrix between all pairs point. Implement this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_matrix(positions):\n",
    "    \"\"\" Return a L2-distance square matrix between all pairs.\n",
    "        If you can, avoid using for-loops. Vectorise all computations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        positions : a Tensor with shape [n_points, n_dims]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        l2_matrix : a Tensor with shape [n_points, n_points]\n",
    "            A matrix containing L2 distance between all pairs of point:\n",
    "            ||x - y||^2\n",
    "    \"\"\"\n",
    "    n_obs, dim = positions.size()\n",
    "\n",
    "    # Duplicate data along 0th dimension\n",
    "    xk = positions.unsqueeze(0).expand(n_obs, n_obs, dim)\n",
    "\n",
    "    # Duplicate data along 1st dimension\n",
    "    xl = positions.unsqueeze(1).expand(n_obs, n_obs, dim)\n",
    "\n",
    "    # (xk - xl) will compute all pairwise differences\n",
    "    # ((xk - xl)**2.0).sum(2) is the L2-norm\n",
    "    l2_matrix = ((xk - xl)**2.0).sum(2)\n",
    "    return l2_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = LongTensor([[1, 2],\n",
    "                        [3, 4],\n",
    "                        [5, 6]])\n",
    "answer = LongTensor([[ 0,  8, 32],\n",
    "                     [ 8,  0,  8],\n",
    "                     [32,  8,  0]])\n",
    "assert torch.eq(compute_distance_matrix(positions), answer).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6. Implementing Forward Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to implement the full forward function. We are given $p_{ij}$ and we've already implemented the KL-divergence. All that's left now is to implement $q_{ij}$\n",
    "\n",
    "$$\n",
    "q_{ij}={\\frac {(1+\\lVert \\mathbf {y} _{i}-\\mathbf {y} _{j}\\rVert ^{2})^{-1}}{\\sum _{k\\neq l}(1+\\lVert \\mathbf {y} _{k}-\\mathbf {y} _{l}\\rVert ^{2})^{-1}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSNE(nn.Module):\n",
    "    def __init__(self, n_points, n_dims=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_points = n_points\n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        # Contains embedding matrix with dimension [n_points, n_dims]\n",
    "        # self.embeddings.weight will return this entire whole matrix\n",
    "        # self.embeddings(LongTensor([4])) will return the positions\n",
    "        # of the fourth object y_4.\n",
    "        \n",
    "        self.embeddings = nn.Embedding(n_points, n_dims)\n",
    "\n",
    "    def forward(self, pij, i, j):\n",
    "        \"\"\" Forward propgation function\n",
    "        \n",
    "            The inputs are a batch of objects with probabilities\n",
    "            pij. As an example, pij[5] is the joint probability \n",
    "            of object i[5] and object j[5]. In the 2D space,\n",
    "            we want object i[5] to have positions self.embeddings[i[5]].\n",
    "        \n",
    "            Parameters\n",
    "            ----------\n",
    "            pij : [batch_size]\n",
    "            i : [batch_size]\n",
    "            j : [batch_size]\n",
    "            \n",
    "            Returns\n",
    "            -------\n",
    "            loss : scalar\n",
    "                The KL loss\n",
    "        \"\"\"\n",
    "        # Get  for all points\n",
    "        positions = self.embeddings.weight\n",
    "\n",
    "        # Compute squared pairwise distances\n",
    "        # Dimension of l2_matrix is (n_points, n_points)\n",
    "        l2_matrix = compute_distance_matrix(positions)\n",
    "\n",
    "        # Compute the demoninator (a single number)\n",
    "        demoninator = (1 + l2_matrix).pow(-1).sum() - self.n_points\n",
    "\n",
    "        # Compute the numerator\n",
    "        xi = self.embeddings(i)\n",
    "        xj = self.embeddings(j)\n",
    "        numerator = ((1 + (xi - xj)**2).sum(1)).pow(-1)\n",
    "\n",
    "        # This probability is the probability of picking the (i, j)\n",
    "        # relationship out of N^2 other possible pairs in the 2D embedding.\n",
    "        qij = numerator / demoninator\n",
    "\n",
    "        # Compute KLD\n",
    "        return kl_loss(pij, qij)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(batch_size, *data):\n",
    "    \"\"\" Yield successive n-sized chunks from data. \"\"\"\n",
    "\n",
    "    endpoints = []\n",
    "    start = 0\n",
    "    for stop in range(0, len(data[0]), batch_size):\n",
    "        if stop > start:\n",
    "            endpoints.append((start, stop))\n",
    "        start = stop\n",
    "\n",
    "    np.random.shuffle(endpoints)\n",
    "    for start, stop in endpoints:\n",
    "        yield [x[start: stop] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "n_epochs = 20\n",
    "\n",
    "model = TSNE(n_points=n, n_dims=2)\n",
    "model = model.to(gpu)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    total = 0\n",
    "    for itr, datas in enumerate(chunks(batch_size, pij, i, j)):\n",
    "        datas = [torch.from_numpy(data) for data in datas]\n",
    "        datas = [data.to(gpu) for data in datas]\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(*datas)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total += loss.data.item()\n",
    "    msg = 'Train Epoch: {} \\tLoss: {:.6e}'\n",
    "    msg = msg.format(epoch, total / (len(i) * 1.0))\n",
    "    print(msg)\n",
    "    \n",
    "    # visualise\n",
    "    embed = model.embeddings.weight.cpu().data.numpy()\n",
    "    ax = plt.figure(figsize=(5,5))\n",
    "    scatter = plt.scatter(embed[:, 0], embed[:, 1], c=y * 1.0 / y.max(), alpha=0.8)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('figures/scatter_{:03d}.png'.format(epoch), bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!convert -loop 0 -delay 20 figures/scatter_0* viz.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('<img src=\"viz.gif\">')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
